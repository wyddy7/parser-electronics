"""–î–∏–∞–≥–Ω–æ—Å—Ç–∏—á–µ—Å–∫–∏–π —Å–∫—Ä–∏–ø—Ç –¥–ª—è keysight-technologies.ru"""
import asyncio
import sys
from pathlib import Path
import urllib.parse

# –ò—Å–ø—Ä–∞–≤–ª—è–µ–º –∫–æ–¥–∏—Ä–æ–≤–∫—É –¥–ª—è Windows
if sys.platform == 'win32':
    import io
    sys.stdout = io.TextIOWrapper(sys.stdout.buffer, encoding='utf-8')
    sys.stderr = io.TextIOWrapper(sys.stderr.buffer, encoding='utf-8')

# –î–æ–±–∞–≤–ª—è–µ–º –ø—É—Ç—å –∫ src
sys.path.insert(0, str(Path(__file__).parent.parent / 'src'))

from config_loader import ConfigLoader
from logger import configure_logging
from parsers.factory import create_async_parser

async def test_debug():
    # 1. –ù–∞—Å—Ç—Ä–æ–π–∫–∞
    config_path = Path(__file__).parent.parent / 'config.yaml'
    config_loader = ConfigLoader(str(config_path))
    parser_config = config_loader.get_parser_config('keysight_technologies')
    search_config = config_loader.get_search_config()
    
    logging_config = config_loader.get_logging_config()
    logging_config['level'] = 'INFO'
    log = configure_logging(logging_config)
    
    parser = create_async_parser('keysight_technologies', parser_config, log, search_config)
    
    # 2. –¢–µ—Å—Ç–æ–≤—ã–π –∑–∞–ø—Ä–æ—Å
    query = "Agilent E4418B" # –ü–æ–ø—É–ª—è—Ä–Ω—ã–π –æ—Å—Ü–∏–ª–ª–æ–≥—Ä–∞—Ñ
    print(f"\n{'='*80}")
    print(f"üîç –¢–µ—Å—Ç–∏—Ä—É–µ–º –∑–∞–ø—Ä–æ—Å: {query}")
    
    async with parser:
        # 3. –ü—Ä–æ–≤–µ—Ä–∫–∞ URL –∏ —Å—ã—Ä–æ–≥–æ –æ—Ç–≤–µ—Ç–∞
        normalized = parser._normalize_search_query(query)
        search_url = parser.search_url_template.format(query=urllib.parse.quote(normalized))
        print(f"URL: {search_url}")
        
        response = await parser._make_request_with_retry(search_url)
        if not response:
            print("‚ùå –ù–µ—Ç –æ—Ç–≤–µ—Ç–∞ –æ—Ç —Å–µ—Ä–≤–µ—Ä–∞")
            return

        print(f"–°—Ç–∞—Ç—É—Å –æ—Ç–≤–µ—Ç–∞: {response.status_code}")
        
        from bs4 import BeautifulSoup
        soup = BeautifulSoup(response.text, 'lxml')
        
        # 4. –ü—Ä–æ–≤–µ—Ä–∫–∞ —Å–µ–ª–µ–∫—Ç–æ—Ä–æ–≤ (–û–∂–∏–¥–∞–Ω–∏–µ vs –†–µ–∞–ª—å–Ω–æ—Å—Ç—å)
        print(f"\nüïµÔ∏è –ü—Ä–æ–≤–µ—Ä–∫–∞ —Å–µ–ª–µ–∫—Ç–æ—Ä–æ–≤:")
        
        # –¢–µ–∫—É—â–∏–π —Å–µ–ª–µ–∫—Ç–æ—Ä
        current_selector = '.products-block.row > .product-layout.product-grid'
        items = soup.select(current_selector)
        print(f"  [–¢–µ–∫—É—â–∏–π] '{current_selector}': {len(items)} —Ç–æ–≤–∞—Ä–æ–≤")
        
        # –ê–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤—ã
        alternatives = [
            '.product-layout',
            '.product-thumb',
            '.products-block .product-layout',
            'div[class*="product-layout"]'
        ]
        
        for alt in alternatives:
            count = len(soup.select(alt))
            print(f"  [–ê–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤–∞] '{alt}': {count} —Ç–æ–≤–∞—Ä–æ–≤")
            
        # 5. –ê–Ω–∞–ª–∏–∑ –≤—Å–µ—Ö –Ω–∞–π–¥–µ–Ω–Ω—ã—Ö —Ç–æ–≤–∞—Ä–æ–≤
        product_containers = soup.select('.product-layout') or soup.select('.product-thumb')
        
        if product_containers:
            print(f"\nüì¶ –ê–Ω–∞–ª–∏–∑ –Ω–∞–π–¥–µ–Ω–Ω—ã—Ö —Ç–æ–≤–∞—Ä–æ–≤ ({len(product_containers)} —à—Ç.):")
            
            for idx, container in enumerate(product_containers, 1):
                print(f"\n  --- –¢–æ–≤–∞—Ä #{idx} ---")
                # –ù–∞–∑–≤–∞–Ω–∏–µ
                name_elem = container.select_one('.product-thumb__name')
                name = name_elem.get_text(strip=True) if name_elem else "–ë–µ–∑ –Ω–∞–∑–≤–∞–Ω–∏—è"
                print(f"  –ù–∞–∑–≤–∞–Ω–∏–µ: {name}")
                
                # –¶–µ–Ω–∞
                price_elem = container.select_one('.price')
                price = price_elem.get_text(strip=True) if price_elem else "–ë–µ–∑ —Ü–µ–Ω—ã"
                print(f"  –¶–µ–Ω–∞: {price}")
                
                if idx == 1:
                     print(f"   HTML (–ø–µ—Ä–≤—ã–µ 200 —Å–∏–º–≤–æ–ª–æ–≤): {str(container)[:200]}...")
                    
        else:
            print("\n‚ùå –¢–æ–≤–∞—Ä—ã –≤–æ–æ–±—â–µ –Ω–µ –Ω–∞–π–¥–µ–Ω—ã. –°—Ç—Ä—É–∫—Ç—É—Ä–∞ —Å—Ç—Ä–∞–Ω–∏—Ü—ã:")
            body_classes = soup.body['class'] if soup.body and soup.body.has_attr('class') else "–ù–µ—Ç –∫–ª–∞—Å—Å–æ–≤"
            print(f"   Body classes: {body_classes}")
            # –ü–æ–∫–∞–∑–∞—Ç—å –æ—Å–Ω–æ–≤–Ω—ã–µ –∫–æ–Ω—Ç–µ–π–Ω–µ—Ä—ã
            main_divs = [d.get('class') for d in soup.find_all('div', limit=10) if d.get('class')]
            print(f"   –ü–µ—Ä–≤—ã–µ 10 div –∫–ª–∞—Å—Å–æ–≤: {main_divs}")

    # 6. –ü–æ–ª–Ω—ã–π —Ç–µ—Å—Ç —á–µ—Ä–µ–∑ search_product
    print(f"\n{'='*80}")
    print(f"üß™ –¢–µ—Å—Ç parser.search_product('{query}'):")
    
    async with parser:
        result = await parser.search_product(query)
        print(f"\n–†–µ–∑—É–ª—å—Ç–∞—Ç: {result}")
        if result:
            print(f"‚úÖ –¢–æ–≤–∞—Ä –Ω–∞–π–¥–µ–Ω!")
            print(f"   –ù–∞–∑–≤–∞–Ω–∏–µ: {result.get('name')}")
            print(f"   –¶–µ–Ω–∞: {result.get('price')}")
            print(f"   URL: {result.get('url')}")
        else:
            print(f"‚ùå –¢–æ–≤–∞—Ä –ù–ï –Ω–∞–π–¥–µ–Ω —á–µ—Ä–µ–∑ search_product")

if __name__ == '__main__':
    asyncio.run(test_debug())

